{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your data\n",
    "# df = pd.read_csv('your_injury_analysis_data.csv')\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Prepare the data for modeling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define feature sets\n",
    "    predictor_features = [\n",
    "        'average_weather_before', 'average_humidity_before', 'average_wind_before',\n",
    "        'most_common_surface', 'most_common_roof', 'average_snaps_before',\n",
    "        'sum_travel_magnitude', 'sum_tz_diff_magnitude', 'sum_elevation_difference',\n",
    "        'prev_weather', 'prev_humidity', 'prev_wind', 'prev_surface', 'prev_roof',\n",
    "        'prev_snaps', 'prev_travel_magnitude', 'prev_is_international',\n",
    "        'prev_elevation_difference', 'prev_travel_direction', 'prev_elevation_difference_abs_m'\n",
    "    ]\n",
    "    \n",
    "    performance_features = [\n",
    "        'pass_cmp_x', 'pass_att', 'pass_yds', 'pass_td', 'pass_int', 'pass_sacked_x',\n",
    "        'pass_sacked_yds', 'pass_long', 'pass_rating', 'rush_att_x', 'rush_yds_x',\n",
    "        'rush_td_x', 'rush_long', 'targets_x', 'rec_x', 'rec_yds_x', 'rec_td_x',\n",
    "        'rec_long', 'fumbles', 'fumbles_lost', 'def_int_x', 'def_int_yds',\n",
    "        'def_int_td', 'def_int_long', 'pass_defended', 'sacks_x', 'tackles_combined_x',\n",
    "        'tackles_solo', 'tackles_assists', 'tackles_loss', 'qb_hits', 'fumbles_rec',\n",
    "        'fumbles_rec_yds', 'fumbles_rec_td', 'fumbles_forced', 'xpm', 'xpa', 'fgm',\n",
    "        'fga', 'punt', 'punt_yds', 'punt_yds_per_punt', 'punt_long', 'kick_ret',\n",
    "        'kick_ret_yds', 'kick_ret_yds_per_ret', 'kick_ret_td', 'kick_ret_long',\n",
    "        'punt_ret', 'punt_ret_yds', 'punt_ret_yds_per_ret', 'punt_ret_td',\n",
    "        'punt_ret_long', 'pass_first_down', 'pass_first_down_pct', 'pass_target_yds',\n",
    "        'pass_tgt_yds_per_att', 'pass_air_yds', 'pass_air_yds_per_cmp',\n",
    "        'pass_air_yds_per_att', 'pass_yac', 'pass_yac_per_cmp', 'pass_drops',\n",
    "        'pass_drop_pct', 'pass_poor_throws', 'pass_poor_throw_pct', 'pass_sacked_y',\n",
    "        'pass_blitzed', 'pass_hurried', 'pass_hits', 'pass_pressured',\n",
    "        'pass_pressured_pct', 'rush_scrambles', 'rush_scrambles_yds_per_att',\n",
    "        'rush_first_down', 'rush_yds_before_contact', 'rush_yds_bc_per_rush',\n",
    "        'rush_yac', 'rush_yac_per_rush', 'rush_broken_tackles',\n",
    "        'rush_broken_tackles_per_rush', 'rec_first_down', 'rec_air_yds',\n",
    "        'rec_air_yds_per_rec', 'rec_yac', 'rec_yac_per_rec', 'rec_adot',\n",
    "        'rec_broken_tackles', 'rec_broken_tackles_per_rec', 'rec_drops',\n",
    "        'rec_drop_pct', 'rec_target_int', 'rec_pass_rating', 'def_targets',\n",
    "        'def_cmp', 'def_cmp_perc', 'def_cmp_yds', 'def_yds_per_cmp',\n",
    "        'def_yds_per_target', 'def_cmp_td', 'def_pass_rating', 'def_tgt_yds_per_att',\n",
    "        'def_air_yds', 'def_yac', 'blitzes', 'qb_hurry', 'qb_knockdown', 'pressures',\n",
    "        'tackles_missed', 'tackles_missed_pct', 'avg_snap_count_after'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    predictor_features = [col for col in predictor_features if col in df.columns]\n",
    "    performance_features = [col for col in performance_features if col in df.columns]\n",
    "    \n",
    "    # Create binary target for injury prediction\n",
    "    # We'll predict injury severity based on game_status\n",
    "    df['injury_severity'] = df['game_status'].map({\n",
    "        'Out': 2,           # Severe injury\n",
    "        'Doubtful': 1,      # Moderate injury\n",
    "        'Questionable': 1,  # Moderate injury\n",
    "        'Probable': 0       # Minor/no injury\n",
    "    })\n",
    "    \n",
    "    # Fill missing values\n",
    "    df[predictor_features] = df[predictor_features].fillna(df[predictor_features].median())\n",
    "    df[performance_features] = df[performance_features].fillna(df[performance_features].median())\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_cols = ['most_common_surface', 'most_common_roof', \n",
    "                       'prev_surface', 'prev_roof', 'prev_travel_direction']\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    return df, predictor_features, performance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59147085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL 1: Predict Injury Risk\n",
    "# ============================================================================\n",
    "\n",
    "def model_injury_risk(df, predictor_features):\n",
    "    \"\"\"\n",
    "    Model 1: Predict injury risk based on pre-injury features.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL 1: INJURY RISK PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[predictor_features]\n",
    "    y = df['injury_severity']  # Multi-class: 0=minor, 1=moderate, 2=severe\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train models\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Train\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate\n",
    "        print(f\"Accuracy: {model.score(X_test_scaled, y_test):.3f}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, \n",
    "              target_names=['Minor', 'Moderate', 'Severe']))\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': predictor_features,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"\\nTop 10 Most Important Features for Injury Prediction:\")\n",
    "            print(feature_importance.head(10).to_string(index=False))\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(feature_importance.head(10)['feature'], \n",
    "                    feature_importance.head(10)['importance'])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f'{name} - Top 10 Injury Risk Features')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': model,\n",
    "                'feature_importance': feature_importance,\n",
    "                'scaler': scaler\n",
    "            }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: Predict Post-Injury Performance\n",
    "# ============================================================================\n",
    "\n",
    "def model_post_injury_performance(df, predictor_features, performance_features):\n",
    "    \"\"\"\n",
    "    Model 2: Predict post-injury performance based on injury characteristics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL 2: POST-INJURY PERFORMANCE PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use injury features as predictors\n",
    "    injury_features = ['injury_lcoation', 'practice_status', 'game_status', 'position']\n",
    "    \n",
    "    # Prepare data\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Encode injury features\n",
    "    le_injury = LabelEncoder()\n",
    "    df_model['injury_lcoation_encoded'] = le_injury.fit_transform(\n",
    "        df_model['injury_lcoation'].fillna('Unknown').astype(str)\n",
    "    )\n",
    "    \n",
    "    le_practice = LabelEncoder()\n",
    "    df_model['practice_status_encoded'] = le_practice.fit_transform(\n",
    "        df_model['practice_status'].fillna('Unknown').astype(str)\n",
    "    )\n",
    "    \n",
    "    le_position = LabelEncoder()\n",
    "    df_model['position_encoded'] = le_position.fit_transform(\n",
    "        df_model['position'].fillna('Unknown').astype(str)\n",
    "    )\n",
    "    \n",
    "    # Combine features\n",
    "    X_features = ['injury_lcoation_encoded', 'practice_status_encoded', 'position_encoded']\n",
    "    X = df_model[X_features]\n",
    "    \n",
    "    performance_results = {}\n",
    "    \n",
    "    # Predict each performance metric\n",
    "    print(f\"\\nPredicting {len(performance_features)} performance metrics...\")\n",
    "    \n",
    "    for i, target_col in enumerate(performance_features[:10]):  # Limit to first 10 for demo\n",
    "        if target_col not in df_model.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{target_col}...\")\n",
    "        \n",
    "        y = df_model[target_col]\n",
    "        \n",
    "        # Remove rows where target is NaN\n",
    "        valid_idx = y.notna()\n",
    "        X_clean = X[valid_idx]\n",
    "        y_clean = y[valid_idx]\n",
    "        \n",
    "        if len(y_clean) < 20:  # Need enough data\n",
    "            continue\n",
    "        \n",
    "        # Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_clean, y_clean, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"  MSE: {mse:.3f}, R²: {r2:.3f}\")\n",
    "        \n",
    "        performance_results[target_col] = {\n",
    "            'model': model,\n",
    "            'mse': mse,\n",
    "            'r2': r2,\n",
    "            'scaler': scaler,\n",
    "            'feature_importance': dict(zip(X_features, model.feature_importances_))\n",
    "        }\n",
    "    \n",
    "    return performance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: Integrated Analysis - Connecting Injury Risk to Performance\n",
    "# ============================================================================\n",
    "\n",
    "def integrated_analysis(df, predictor_features, performance_features):\n",
    "    \"\"\"\n",
    "    Model 3: Connect injury risk factors to post-injury performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL 3: INTEGRATED ANALYSIS\")\n",
    "    print(\"Connecting Injury Risk Factors to Performance Impact\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 3.1: Cluster players by injury risk profile\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    X_risk = df[predictor_features].fillna(df[predictor_features].median())\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_risk)\n",
    "    \n",
    "    # Find optimal clusters\n",
    "    inertias = []\n",
    "    for k in range(2, 10):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Elbow method visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(2, 10), inertias, 'bo-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal Clusters')\n",
    "    plt.show()\n",
    "    \n",
    "    # Choose k (e.g., 4)\n",
    "    k = 4\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    df['risk_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # 3.2: Analyze performance by risk cluster\n",
    "    print(\"\\nPerformance by Risk Cluster:\")\n",
    "    cluster_performance = {}\n",
    "    \n",
    "    for cluster in range(k):\n",
    "        cluster_mask = df['risk_cluster'] == cluster\n",
    "        cluster_size = cluster_mask.sum()\n",
    "        \n",
    "        print(f\"\\nCluster {cluster} (n={cluster_size}):\")\n",
    "        \n",
    "        # Get average performance metrics for this cluster\n",
    "        cluster_avg = df.loc[cluster_mask, performance_features].mean()\n",
    "        \n",
    "        # Top 5 performance metrics where this cluster differs most from overall mean\n",
    "        overall_avg = df[performance_features].mean()\n",
    "        diff = (cluster_avg - overall_avg).abs()\n",
    "        top_diffs = diff.nlargest(5)\n",
    "        \n",
    "        print(\"  Most affected performance metrics:\")\n",
    "        for metric, diff_val in top_diffs.items():\n",
    "            cluster_val = cluster_avg[metric]\n",
    "            overall_val = overall_avg[metric]\n",
    "            pct_change = ((cluster_val - overall_val) / overall_val * 100) if overall_val != 0 else 0\n",
    "            print(f\"    {metric}: {cluster_val:.2f} vs {overall_val:.2f} ({pct_change:+.1f}%)\")\n",
    "    \n",
    "    # 3.3: Key injury risk factors analysis\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"KEY FINDINGS: Injury Risk Factors\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Correlation analysis\n",
    "    injury_correlations = {}\n",
    "    for feature in predictor_features:\n",
    "        corr = df[feature].corr(df['injury_severity'])\n",
    "        if not pd.isna(corr):\n",
    "            injury_correlations[feature] = abs(corr)\n",
    "    \n",
    "    top_corrs = pd.Series(injury_correlations).nlargest(10)\n",
    "    print(\"\\nTop 10 factors correlated with injury severity:\")\n",
    "    for feature, corr in top_corrs.items():\n",
    "        direction = \"increases\" if df[feature].corr(df['injury_severity']) > 0 else \"decreases\"\n",
    "        print(f\"  {feature}: {corr:.3f} ({direction} risk)\")\n",
    "    \n",
    "    # 3.4: Performance impact by injury type\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"Performance Impact by Injury Type\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if 'injury_lcoation' in df.columns:\n",
    "        injury_locations = df['injury_lcoation'].value_counts().head(5).index\n",
    "        \n",
    "        for location in injury_locations:\n",
    "            location_mask = df['injury_lcoation'] == location\n",
    "            if location_mask.sum() > 10:  # Enough samples\n",
    "                print(f\"\\n{location}:\")\n",
    "                \n",
    "                # Compare performance before vs average\n",
    "                location_perf = df.loc[location_mask, performance_features].mean()\n",
    "                overall_perf = df[performance_features].mean()\n",
    "                \n",
    "                # Find biggest differences\n",
    "                perf_diff = (location_perf - overall_perf) / overall_perf * 100\n",
    "                perf_diff = perf_diff.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "                \n",
    "                biggest_drops = perf_diff.nsmallest(3)\n",
    "                biggest_gains = perf_diff.nlargest(3)\n",
    "                \n",
    "                print(\"  Biggest performance drops:\")\n",
    "                for metric, pct in biggest_drops.items():\n",
    "                    print(f\"    {metric}: {pct:+.1f}%\")\n",
    "                \n",
    "                print(\"  Biggest performance gains:\")\n",
    "                for metric, pct in biggest_gains.items():\n",
    "                    print(f\"    {metric}: {pct:+.1f}%\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb883157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_injury_analysis_pipeline(df):\n",
    "    \"\"\"\n",
    "    Run the complete injury analysis pipeline.\n",
    "    \"\"\"\n",
    "    print(\"INJURY ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    df_prepared, predictor_features, performance_features = prepare_data(df)\n",
    "    print(f\"Data prepared: {len(df_prepared)} samples\")\n",
    "    print(f\"Predictor features: {len(predictor_features)}\")\n",
    "    print(f\"Performance features: {len(performance_features)}\")\n",
    "    \n",
    "    # Step 2: Model 1 - Injury Risk Prediction\n",
    "    injury_risk_results = model_injury_risk(df_prepared, predictor_features)\n",
    "    \n",
    "    # Step 3: Model 2 - Post-Injury Performance Prediction\n",
    "    performance_results = model_post_injury_performance(\n",
    "        df_prepared, predictor_features, performance_features\n",
    "    )\n",
    "    \n",
    "    # Step 4: Model 3 - Integrated Analysis\n",
    "    df_analyzed = integrated_analysis(df_prepared, predictor_features, performance_features)\n",
    "    \n",
    "    # Step 5: Summary and Recommendations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key takeaways from Model 1\n",
    "    print(\"\\n1. KEY INJURY RISK FACTORS:\")\n",
    "    rf_importance = injury_risk_results.get('Random Forest', {}).get('feature_importance')\n",
    "    if rf_importance is not None:\n",
    "        top_risks = rf_importance.head(5)\n",
    "        for _, row in top_risks.iterrows():\n",
    "            print(f\"   • {row['feature']}: Importance = {row['importance']:.3f}\")\n",
    "    \n",
    "    # Key takeaways from Model 2\n",
    "    print(\"\\n2. PERFORMANCE IMPACT FINDINGS:\")\n",
    "    if performance_results:\n",
    "        # Sort by R² to find most predictable metrics\n",
    "        predictable_metrics = sorted(\n",
    "            [(metric, results['r2']) for metric, results in performance_results.items()],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        print(\"   Most predictable post-injury metrics:\")\n",
    "        for metric, r2 in predictable_metrics:\n",
    "            print(f\"   • {metric}: R² = {r2:.3f}\")\n",
    "    \n",
    "    # Practical recommendations\n",
    "    print(\"\\n3. PRACTICAL RECOMMENDATIONS:\")\n",
    "    print(\"   a) Monitor high-risk factors identified (travel, workload, environment)\")\n",
    "    print(\"   b) Adjust training based on injury location and severity predictions\")\n",
    "    print(\"   c) Use performance predictions to set realistic return expectations\")\n",
    "    print(\"   d) Consider player position in risk assessment\")\n",
    "    \n",
    "    return {\n",
    "        'data': df_analyzed,\n",
    "        'injury_risk_models': injury_risk_results,\n",
    "        'performance_models': performance_results,\n",
    "        'predictor_features': predictor_features,\n",
    "        'performance_features': performance_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_visualizations(results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the analysis.\n",
    "    \"\"\"\n",
    "    df = results['data']\n",
    "    predictor_features = results['predictor_features']\n",
    "    \n",
    "    # 1. Injury Severity Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['injury_severity'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.xlabel('Injury Severity (0=Minor, 1=Moderate, 2=Severe)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Injury Severity')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Top Predictor Features Heatmap\n",
    "    if len(predictor_features) >= 5:\n",
    "        top_features = predictor_features[:10]  # Top 10 features\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        correlation_matrix = df[top_features + ['injury_severity']].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Heatmap: Predictor Features vs Injury Severity')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. Performance Before vs After Injury (for top 3 positions)\n",
    "    if 'position' in df.columns and 'avg_snap_count_after' in df.columns:\n",
    "        top_positions = df['position'].value_counts().head(3).index\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        for idx, position in enumerate(top_positions):\n",
    "            pos_mask = df['position'] == position\n",
    "            if pos_mask.sum() > 0:\n",
    "                axes[idx].scatter(\n",
    "                    df.loc[pos_mask, 'average_snaps_before'],\n",
    "                    df.loc[pos_mask, 'avg_snap_count_after'],\n",
    "                    alpha=0.6\n",
    "                )\n",
    "                axes[idx].plot([0, 100], [0, 100], 'r--', alpha=0.5)  # Reference line\n",
    "                axes[idx].set_xlabel('Average Snaps Before Injury')\n",
    "                axes[idx].set_ylabel('Average Snaps After Injury')\n",
    "                axes[idx].set_title(f'{position} - Snaps Before vs After')\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93f273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    # df = pd.read_csv('your_injury_data.csv')\n",
    "    \n",
    "    # Run the complete pipeline\n",
    "    analysis_results = run_injury_analysis_pipeline(df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(analysis_results)\n",
    "    \n",
    "    # Save results\n",
    "    analysis_results['data'].to_csv('injury_analysis_results.csv', index=False)\n",
    "    print(\"\\nAnalysis complete! Results saved to 'injury_analysis_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf30abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
