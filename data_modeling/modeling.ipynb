{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680e14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743d3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Prepare the data for modeling with proper handling of mixed data types.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define feature sets\n",
    "    predictor_features = [\n",
    "        'average_weather_before',\n",
    "        'most_common_surface', 'most_common_roof', 'average_snaps_before',\n",
    "        'sum_travel_magnitude', 'sum_tz_diff_magnitude', 'sum_elevation_difference',\n",
    "        'prev_weather', 'prev_wind', 'prev_surface', 'prev_roof',\n",
    "        'prev_snaps', 'prev_travel_magnitude', 'prev_is_international',\n",
    "        'prev_elevation_difference', 'prev_travel_direction', 'prev_elevation_difference_abs_m'\n",
    "    ]\n",
    "    \n",
    "    performance_features = [\n",
    "        'pass_cmp_x', 'pass_att', 'pass_yds', 'pass_td', 'pass_int', 'pass_sacked_x',\n",
    "        'pass_sacked_yds', 'pass_long', 'pass_rating', 'rush_att_x', 'rush_yds_x',\n",
    "        'rush_td_x', 'rush_long', 'targets_x', 'rec_x', 'rec_yds_x', 'rec_td_x',\n",
    "        'rec_long', 'fumbles', 'fumbles_lost', 'def_int_x', 'def_int_yds',\n",
    "        'def_int_td', 'def_int_long', 'pass_defended', 'sacks_x', 'tackles_combined_x',\n",
    "        'tackles_solo', 'tackles_assists', 'tackles_loss', 'qb_hits', 'fumbles_rec',\n",
    "        'fumbles_rec_yds', 'fumbles_rec_td', 'fumbles_forced', 'xpm', 'xpa', 'fgm',\n",
    "        'fga', 'punt', 'punt_yds', 'punt_yds_per_punt', 'punt_long', 'kick_ret',\n",
    "        'kick_ret_yds', 'kick_ret_yds_per_ret', 'kick_ret_td', 'kick_ret_long',\n",
    "        'punt_ret', 'punt_ret_yds', 'punt_ret_yds_per_ret', 'punt_ret_td',\n",
    "        'punt_ret_long', 'pass_first_down', 'pass_first_down_pct', 'pass_target_yds',\n",
    "        'pass_tgt_yds_per_att', 'pass_air_yds', 'pass_air_yds_per_cmp',\n",
    "        'pass_air_yds_per_att', 'pass_yac', 'pass_yac_per_cmp', 'pass_drops',\n",
    "        'pass_drop_pct', 'pass_poor_throws', 'pass_poor_throw_pct', 'pass_sacked_y',\n",
    "        'pass_blitzed', 'pass_hurried', 'pass_hits', 'pass_pressured',\n",
    "        'pass_pressured_pct', 'rush_scrambles', 'rush_scrambles_yds_per_att',\n",
    "        'rush_first_down', 'rush_yds_before_contact', 'rush_yds_bc_per_rush',\n",
    "        'rush_yac', 'rush_yac_per_rush', 'rush_broken_tackles',\n",
    "        'rush_broken_tackles_per_rush', 'rec_first_down', 'rec_air_yds',\n",
    "        'rec_air_yds_per_rec', 'rec_yac', 'rec_yac_per_rec', 'rec_adot',\n",
    "        'rec_broken_tackles', 'rec_broken_tackles_per_rec', 'rec_drops',\n",
    "        'rec_drop_pct', 'rec_target_int', 'rec_pass_rating', 'def_targets',\n",
    "        'def_cmp', 'def_cmp_perc', 'def_cmp_yds', 'def_yds_per_cmp',\n",
    "        'def_yds_per_target', 'def_cmp_td', 'def_pass_rating', 'def_tgt_yds_per_att',\n",
    "        'def_air_yds', 'def_yac', 'blitzes', 'qb_hurry', 'qb_knockdown', 'pressures',\n",
    "        'tackles_missed', 'tackles_missed_pct', 'avg_snap_count_after'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    predictor_features = [col for col in predictor_features if col in df.columns]\n",
    "    performance_features = [col for col in performance_features if col in df.columns]\n",
    "    \n",
    "    print(f\"Found {len(predictor_features)} predictor features\")\n",
    "    print(f\"Found {len(performance_features)} performance features\")\n",
    "    \n",
    "    # Create binary target for injury prediction\n",
    "    print(\"\\nCreating injury severity target...\")\n",
    "    if 'game_status' in df.columns:\n",
    "        # Clean game_status column\n",
    "        df['game_status'] = df['game_status'].astype(str).str.strip().str.lower()\n",
    "        \n",
    "        # Map to severity\n",
    "        severity_map = {\n",
    "            'out': 2,\n",
    "            'doubtful': 1,\n",
    "            'questionable': 1,\n",
    "            'probable': 0,\n",
    "            'nan': np.nan,\n",
    "            'none': np.nan\n",
    "        }\n",
    "        \n",
    "        df['injury_severity'] = df['game_status'].map(severity_map)\n",
    "        print(f\"Injury severity distribution:\")\n",
    "        print(df['injury_severity'].value_counts(dropna=False))\n",
    "    else:\n",
    "        print(\"Warning: 'game_status' column not found, cannot create injury severity\")\n",
    "        df['injury_severity'] = np.nan\n",
    "    \n",
    "    # Handle missing values separately for numeric and categorical columns\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    \n",
    "    # Identify numeric vs categorical columns\n",
    "    numeric_predictors = []\n",
    "    categorical_predictors = []\n",
    "    \n",
    "    for col in predictor_features:\n",
    "        if col in df.columns:\n",
    "            # Try to convert to numeric to check\n",
    "            numeric_test = pd.to_numeric(df[col], errors='coerce')\n",
    "            numeric_count = numeric_test.notna().sum()\n",
    "            total_count = df[col].notna().sum()\n",
    "            \n",
    "            if numeric_count > 0 and (numeric_count / total_count > 0.5 if total_count > 0 else False):\n",
    "                numeric_predictors.append(col)\n",
    "            else:\n",
    "                categorical_predictors.append(col)\n",
    "    \n",
    "    print(f\"Numeric predictors: {len(numeric_predictors)}\")\n",
    "    print(f\"Categorical predictors: {len(categorical_predictors)}\")\n",
    "    \n",
    "    # Fill missing values for numeric columns\n",
    "    if numeric_predictors:\n",
    "        for col in numeric_predictors:\n",
    "            # Extract numbers from strings like \"56%\", \"88%\", etc.\n",
    "            if df[col].dtype == object:\n",
    "                # Clean the column first\n",
    "                df[col] = df[col].astype(str).str.extract(r'([-+]?\\d*\\.?\\d+)')[0]\n",
    "            \n",
    "            # Convert to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Fill missing with median\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # Fill missing values for categorical columns\n",
    "    if categorical_predictors:\n",
    "        for col in categorical_predictors:\n",
    "            # Fill with mode (most common value)\n",
    "            if df[col].notna().sum() > 0:\n",
    "                mode_val = df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown'\n",
    "            else:\n",
    "                mode_val = 'Unknown'\n",
    "            \n",
    "            df[col] = df[col].fillna(mode_val)\n",
    "    \n",
    "    # Fill missing values for performance features\n",
    "    for col in performance_features:\n",
    "        if col in df.columns:\n",
    "            # Clean if string\n",
    "            if df[col].dtype == object:\n",
    "                df[col] = df[col].astype(str).str.extract(r'([-+]?\\d*\\.?\\d+)')[0]\n",
    "            \n",
    "            # Convert to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Fill missing with median\n",
    "            if df[col].notna().sum() > 0:\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    print(\"\\nEncoding categorical features...\")\n",
    "    \n",
    "    # Identify which categorical columns to encode\n",
    "    categorical_cols = ['most_common_surface', 'most_common_roof', \n",
    "                       'prev_surface', 'prev_roof', 'prev_travel_direction']\n",
    "    \n",
    "    # Filter to only columns that exist and are categorical\n",
    "    categorical_cols = [col for col in categorical_cols if col in df.columns and col in categorical_predictors]\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        try:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"  Encoded {col}: {len(le.classes_)} categories\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not encode {col}: {e}\")\n",
    "            # Keep as is or create dummy variables\n",
    "    \n",
    "    print(f\"\\nFinal prepared data shape: {df.shape}\")\n",
    "    \n",
    "    return df, predictor_features, performance_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59147085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_injury_risk(df, predictor_features):\n",
    "    \"\"\"\n",
    "    Model 1: Predict injury risk based on pre-injury features.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MODEL 1: INJURY RISK PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check if we have the target column\n",
    "    if 'injury_severity' not in df.columns:\n",
    "        print(\"Warning: 'injury_severity' column not found\")\n",
    "        return {}\n",
    "    \n",
    "    # Check for NaN in target\n",
    "    nan_count = df['injury_severity'].isna().sum()\n",
    "    print(f\"Target column 'injury_severity' has {nan_count} NaN values\")\n",
    "    \n",
    "    # Remove rows with NaN in target\n",
    "    df_clean = df.dropna(subset=['injury_severity']).copy()\n",
    "    \n",
    "    if len(df_clean) == 0:\n",
    "        print(\"Error: No valid injury severity data after removing NaN\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Using {len(df_clean)} samples after removing NaN from target\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df_clean[predictor_features]\n",
    "    y = df_clean['injury_severity']  # Multi-class: 0=minor, 1=moderate, 2=severe\n",
    "    \n",
    "    # Convert y to integer if it's float\n",
    "    if y.dtype == float:\n",
    "        y = y.astype(int)\n",
    "    \n",
    "    print(f\"Target distribution:\")\n",
    "    print(y.value_counts().sort_index())\n",
    "    \n",
    "    # Check if we have enough samples for each class\n",
    "    class_counts = y.value_counts()\n",
    "    min_samples = class_counts.min()\n",
    "    print(f\"Minimum class samples: {min_samples}\")\n",
    "    \n",
    "    if min_samples < 5:\n",
    "        print(\"Warning: Some classes have very few samples, consider collapsing categories\")\n",
    "    \n",
    "    # Split data\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Stratified split failed: {e}\")\n",
    "        print(\"Using regular split instead...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "    non_numeric = X_train.select_dtypes(include=[\"object\"])\n",
    "    print(non_numeric.columns)\n",
    "    print(non_numeric.head())\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train models\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'XGBoost': xgb.XGBClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Train\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Evaluate\n",
    "            print(f\"Accuracy: {model.score(X_test_scaled, y_test):.3f}\")\n",
    "            print(\"Classification Report:\")\n",
    "            print(classification_report(y_test, y_pred, \n",
    "                  target_names=['Minor', 'Moderate', 'Severe']))\n",
    "            \n",
    "            # Feature importance\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': predictor_features,\n",
    "                    'importance': importances\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\nTop 10 Most Important Features for Injury Prediction:\")\n",
    "                print(feature_importance.head(10).to_string(index=False))\n",
    "                \n",
    "                # Plot feature importance\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.barh(feature_importance.head(10)['feature'], \n",
    "                        feature_importance.head(10)['importance'])\n",
    "                plt.xlabel('Importance')\n",
    "                plt.title(f'{name} - Top 10 Injury Risk Features')\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'feature_importance': feature_importance,\n",
    "                    'scaler': scaler\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed26097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL 2: Predict Post-Injury Performance\n",
    "# ============================================================================\n",
    "\n",
    "def model_post_injury_performance(df, predictor_features, performance_features):\n",
    "    \"\"\"\n",
    "    Model 2: Predict post-injury performance based on injury characteristics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL 2: POST-INJURY PERFORMANCE PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use injury features as predictors\n",
    "    injury_features = ['injury_lcoation', 'practice_status', 'game_status', 'position']\n",
    "    \n",
    "    # Prepare data\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Encode injury features\n",
    "    le_injury = LabelEncoder()\n",
    "    df_model['injury_lcoation_encoded'] = le_injury.fit_transform(\n",
    "        df_model['injury_lcoation'].fillna('Unknown').astype(str)\n",
    "    )\n",
    "    \n",
    "    le_practice = LabelEncoder()\n",
    "    df_model['practice_status_encoded'] = le_practice.fit_transform(\n",
    "        df_model['practice_status'].fillna('Unknown').astype(str)\n",
    "    )\n",
    "    \n",
    "    le_position = LabelEncoder()\n",
    "    df_model['position_encoded'] = le_position.fit_transform(\n",
    "        df_model['position'].fillna('Unknown').astype(str)\n",
    "    )\n",
    "    \n",
    "    # Combine features\n",
    "    X_features = ['injury_lcoation_encoded', 'practice_status_encoded', 'position_encoded']\n",
    "    X = df_model[X_features]\n",
    "    \n",
    "    performance_results = {}\n",
    "    \n",
    "    # Predict each performance metric\n",
    "    print(f\"\\nPredicting {len(performance_features)} performance metrics...\")\n",
    "    \n",
    "    for i, target_col in enumerate(performance_features[:10]):  # Limit to first 10 for demo\n",
    "        if target_col not in df_model.columns:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{target_col}...\")\n",
    "        \n",
    "        y = df_model[target_col]\n",
    "        \n",
    "        # Remove rows where target is NaN\n",
    "        valid_idx = y.notna()\n",
    "        X_clean = X[valid_idx]\n",
    "        y_clean = y[valid_idx]\n",
    "        \n",
    "        if len(y_clean) < 20:  # Need enough data\n",
    "            continue\n",
    "        \n",
    "        # Split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_clean, y_clean, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Evaluate\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"  MSE: {mse:.3f}, R²: {r2:.3f}\")\n",
    "        \n",
    "        performance_results[target_col] = {\n",
    "            'model': model,\n",
    "            'mse': mse,\n",
    "            'r2': r2,\n",
    "            'scaler': scaler,\n",
    "            'feature_importance': dict(zip(X_features, model.feature_importances_))\n",
    "        }\n",
    "    \n",
    "    return performance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MODEL 3: Integrated Analysis - Connecting Injury Risk to Performance\n",
    "# ============================================================================\n",
    "\n",
    "def integrated_analysis(df, predictor_features, performance_features):\n",
    "    \"\"\"\n",
    "    Model 3: Connect injury risk factors to post-injury performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL 3: INTEGRATED ANALYSIS\")\n",
    "    print(\"Connecting Injury Risk Factors to Performance Impact\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 3.1: Cluster players by injury risk profile\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    X_risk = df[predictor_features].fillna(df[predictor_features].median())\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_risk)\n",
    "    \n",
    "    # Find optimal clusters\n",
    "    inertias = []\n",
    "    for k in range(2, 10):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Elbow method visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(2, 10), inertias, 'bo-')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal Clusters')\n",
    "    plt.show()\n",
    "    \n",
    "    # Choose k (e.g., 4)\n",
    "    k = 4\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    df['risk_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # 3.2: Analyze performance by risk cluster\n",
    "    print(\"\\nPerformance by Risk Cluster:\")\n",
    "    cluster_performance = {}\n",
    "    \n",
    "    for cluster in range(k):\n",
    "        cluster_mask = df['risk_cluster'] == cluster\n",
    "        cluster_size = cluster_mask.sum()\n",
    "        \n",
    "        print(f\"\\nCluster {cluster} (n={cluster_size}):\")\n",
    "        \n",
    "        # Get average performance metrics for this cluster\n",
    "        cluster_avg = df.loc[cluster_mask, performance_features].mean()\n",
    "        \n",
    "        # Top 5 performance metrics where this cluster differs most from overall mean\n",
    "        overall_avg = df[performance_features].mean()\n",
    "        diff = (cluster_avg - overall_avg).abs()\n",
    "        top_diffs = diff.nlargest(5)\n",
    "        \n",
    "        print(\"  Most affected performance metrics:\")\n",
    "        for metric, diff_val in top_diffs.items():\n",
    "            cluster_val = cluster_avg[metric]\n",
    "            overall_val = overall_avg[metric]\n",
    "            pct_change = ((cluster_val - overall_val) / overall_val * 100) if overall_val != 0 else 0\n",
    "            print(f\"    {metric}: {cluster_val:.2f} vs {overall_val:.2f} ({pct_change:+.1f}%)\")\n",
    "    \n",
    "    # 3.3: Key injury risk factors analysis\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"KEY FINDINGS: Injury Risk Factors\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Correlation analysis\n",
    "    injury_correlations = {}\n",
    "    for feature in predictor_features:\n",
    "        corr = df[feature].corr(df['injury_severity'])\n",
    "        if not pd.isna(corr):\n",
    "            injury_correlations[feature] = abs(corr)\n",
    "    \n",
    "    top_corrs = pd.Series(injury_correlations).nlargest(10)\n",
    "    print(\"\\nTop 10 factors correlated with injury severity:\")\n",
    "    for feature, corr in top_corrs.items():\n",
    "        direction = \"increases\" if df[feature].corr(df['injury_severity']) > 0 else \"decreases\"\n",
    "        print(f\"  {feature}: {corr:.3f} ({direction} risk)\")\n",
    "    \n",
    "    # 3.4: Performance impact by injury type\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"Performance Impact by Injury Type\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if 'injury_lcoation' in df.columns:\n",
    "        injury_locations = df['injury_lcoation'].value_counts().head(5).index\n",
    "        \n",
    "        for location in injury_locations:\n",
    "            location_mask = df['injury_lcoation'] == location\n",
    "            if location_mask.sum() > 10:  # Enough samples\n",
    "                print(f\"\\n{location}:\")\n",
    "                \n",
    "                # Compare performance before vs average\n",
    "                location_perf = df.loc[location_mask, performance_features].mean()\n",
    "                overall_perf = df[performance_features].mean()\n",
    "                \n",
    "                # Find biggest differences\n",
    "                perf_diff = (location_perf - overall_perf) / overall_perf * 100\n",
    "                perf_diff = perf_diff.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "                \n",
    "                biggest_drops = perf_diff.nsmallest(3)\n",
    "                biggest_gains = perf_diff.nlargest(3)\n",
    "                \n",
    "                print(\"  Biggest performance drops:\")\n",
    "                for metric, pct in biggest_drops.items():\n",
    "                    print(f\"    {metric}: {pct:+.1f}%\")\n",
    "                \n",
    "                print(\"  Biggest performance gains:\")\n",
    "                for metric, pct in biggest_gains.items():\n",
    "                    print(f\"    {metric}: {pct:+.1f}%\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb883157",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_injury_analysis_pipeline(df):\n",
    "    \"\"\"\n",
    "    Run the complete injury analysis pipeline.\n",
    "    \"\"\"\n",
    "    print(\"INJURY ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Prepare data\n",
    "    df_prepared, predictor_features, performance_features = prepare_data(df)\n",
    "    print(f\"Data prepared: {len(df_prepared)} samples\")\n",
    "    print(f\"Predictor features: {len(predictor_features)}\")\n",
    "    print(f\"Performance features: {len(performance_features)}\")\n",
    "    \n",
    "    # Step 2: Model 1 - Injury Risk Prediction\n",
    "    injury_risk_results = model_injury_risk(df_prepared, predictor_features)\n",
    "    \n",
    "    # Step 3: Model 2 - Post-Injury Performance Prediction\n",
    "    performance_results = model_post_injury_performance(\n",
    "        df_prepared, predictor_features, performance_features\n",
    "    )\n",
    "    \n",
    "    # Step 4: Model 3 - Integrated Analysis\n",
    "    df_analyzed = integrated_analysis(df_prepared, predictor_features, performance_features)\n",
    "    \n",
    "    # Step 5: Summary and Recommendations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key takeaways from Model 1\n",
    "    print(\"\\n1. KEY INJURY RISK FACTORS:\")\n",
    "    rf_importance = injury_risk_results.get('Random Forest', {}).get('feature_importance')\n",
    "    if rf_importance is not None:\n",
    "        top_risks = rf_importance.head(5)\n",
    "        for _, row in top_risks.iterrows():\n",
    "            print(f\"   • {row['feature']}: Importance = {row['importance']:.3f}\")\n",
    "    \n",
    "    # Key takeaways from Model 2\n",
    "    print(\"\\n2. PERFORMANCE IMPACT FINDINGS:\")\n",
    "    if performance_results:\n",
    "        # Sort by R² to find most predictable metrics\n",
    "        predictable_metrics = sorted(\n",
    "            [(metric, results['r2']) for metric, results in performance_results.items()],\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "        \n",
    "        print(\"   Most predictable post-injury metrics:\")\n",
    "        for metric, r2 in predictable_metrics:\n",
    "            print(f\"   • {metric}: R² = {r2:.3f}\")\n",
    "    \n",
    "    # Practical recommendations\n",
    "    print(\"\\n3. PRACTICAL RECOMMENDATIONS:\")\n",
    "    print(\"   a) Monitor high-risk factors identified (travel, workload, environment)\")\n",
    "    print(\"   b) Adjust training based on injury location and severity predictions\")\n",
    "    print(\"   c) Use performance predictions to set realistic return expectations\")\n",
    "    print(\"   d) Consider player position in risk assessment\")\n",
    "    \n",
    "    return {\n",
    "        'data': df_analyzed,\n",
    "        'injury_risk_models': injury_risk_results,\n",
    "        'performance_models': performance_results,\n",
    "        'predictor_features': predictor_features,\n",
    "        'performance_features': performance_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "505d4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_visualizations(results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for the analysis.\n",
    "    \"\"\"\n",
    "    df = results['data']\n",
    "    predictor_features = results['predictor_features']\n",
    "    \n",
    "    # 1. Injury Severity Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df['injury_severity'].value_counts().sort_index().plot(kind='bar')\n",
    "    plt.xlabel('Injury Severity (0=Minor, 1=Moderate, 2=Severe)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Injury Severity')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Top Predictor Features Heatmap\n",
    "    if len(predictor_features) >= 5:\n",
    "        top_features = predictor_features[:10]  # Top 10 features\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        correlation_matrix = df[top_features + ['injury_severity']].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Correlation Heatmap: Predictor Features vs Injury Severity')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. Performance Before vs After Injury (for top 3 positions)\n",
    "    if 'position' in df.columns and 'avg_snap_count_after' in df.columns:\n",
    "        top_positions = df['position'].value_counts().head(3).index\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        for idx, position in enumerate(top_positions):\n",
    "            pos_mask = df['position'] == position\n",
    "            if pos_mask.sum() > 0:\n",
    "                axes[idx].scatter(\n",
    "                    df.loc[pos_mask, 'average_snaps_before'],\n",
    "                    df.loc[pos_mask, 'avg_snap_count_after'],\n",
    "                    alpha=0.6\n",
    "                )\n",
    "                axes[idx].plot([0, 100], [0, 100], 'r--', alpha=0.5)  # Reference line\n",
    "                axes[idx].set_xlabel('Average Snaps Before Injury')\n",
    "                axes[idx].set_ylabel('Average Snaps After Injury')\n",
    "                axes[idx].set_title(f'{position} - Snaps Before vs After')\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08ad45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab93f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INJURY ANALYSIS PIPELINE\n",
      "============================================================\n",
      "Found 17 predictor features\n",
      "Found 111 performance features\n",
      "\n",
      "Creating injury severity target...\n",
      "Injury severity distribution:\n",
      "injury_severity\n",
      "NaN    280024\n",
      "1.0     17132\n",
      "2.0     10480\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Handling missing values...\n",
      "Numeric predictors: 12\n",
      "Categorical predictors: 5\n",
      "\n",
      "Encoding categorical features...\n",
      "  Encoded most_common_surface: 8 categories\n",
      "  Encoded most_common_roof: 4 categories\n",
      "  Encoded prev_surface: 8 categories\n",
      "  Encoded prev_roof: 4 categories\n",
      "  Encoded prev_travel_direction: 4 categories\n",
      "\n",
      "Final prepared data shape: (307636, 138)\n",
      "Data prepared: 307636 samples\n",
      "Predictor features: 17\n",
      "Performance features: 111\n",
      "============================================================\n",
      "MODEL 1: INJURY RISK PREDICTION\n",
      "============================================================\n",
      "Target column 'injury_severity' has 280024 NaN values\n",
      "Using 27612 samples after removing NaN from target\n",
      "Target distribution:\n",
      "injury_severity\n",
      "1    17132\n",
      "2    10480\n",
      "Name: count, dtype: int64\n",
      "Minimum class samples: 10480\n",
      "\n",
      "Train shape: (22089, 17), Test shape: (5523, 17)\n",
      "Index([], dtype='object')\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [52098, 283130, 282598, 20986, 9599]\n",
      "\n",
      "Training Random Forest...\n",
      "Accuracy: 0.650\n",
      "Classification Report:\n",
      "Error training Random Forest: Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter\n",
      "\n",
      "Training Logistic Regression...\n",
      "Error training Logistic Regression: Input X contains NaN.\n",
      "LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "\n",
      "Training XGBoost...\n",
      "Error training XGBoost: Invalid classes inferred from unique values of `y`.  Expected: [0 1], got [1 2]\n",
      "\n",
      "============================================================\n",
      "MODEL 2: POST-INJURY PERFORMANCE PREDICTION\n",
      "============================================================\n",
      "\n",
      "Predicting 111 performance metrics...\n",
      "\n",
      "pass_cmp_x...\n",
      "  MSE: 8.117, R²: 0.161\n",
      "\n",
      "pass_att...\n",
      "  MSE: 19.405, R²: 0.163\n",
      "\n",
      "pass_yds...\n",
      "  MSE: 992.194, R²: 0.158\n",
      "\n",
      "pass_td...\n",
      "  MSE: 0.043, R²: 0.150\n",
      "\n",
      "pass_int...\n",
      "  MSE: 0.013, R²: 0.144\n",
      "\n",
      "pass_sacked_x...\n",
      "  MSE: 0.108, R²: 0.146\n",
      "\n",
      "pass_sacked_yds...\n",
      "  MSE: 4.561, R²: 0.143\n",
      "\n",
      "pass_long...\n",
      "  MSE: 27.414, R²: 0.156\n",
      "\n",
      "pass_rating...\n",
      "  MSE: 77.245, R²: -0.001\n",
      "\n",
      "rush_att_x...\n",
      "  MSE: 4.413, R²: 0.147\n",
      "\n",
      "============================================================\n",
      "MODEL 3: INTEGRATED ANALYSIS\n",
      "Connecting Injury Risk Factors to Performance Impact\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     32\u001b[39m     df[col] = pd.to_numeric(df[col], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Run the complete pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m analysis_results = \u001b[43mrun_injury_analysis_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Create visualizations\u001b[39;00m\n\u001b[32m     39\u001b[39m create_visualizations(analysis_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrun_injury_analysis_pipeline\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     22\u001b[39m performance_results = model_post_injury_performance(\n\u001b[32m     23\u001b[39m     df_prepared, predictor_features, performance_features\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Step 4: Model 3 - Integrated Analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m df_analyzed = \u001b[43mintegrated_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prepared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperformance_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Step 5: Summary and Recommendations\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mintegrated_analysis\u001b[39m\u001b[34m(df, predictor_features, performance_features)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m, \u001b[32m10\u001b[39m):\n\u001b[32m     26\u001b[39m     kmeans = KMeans(n_clusters=k, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43mkmeans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     inertias.append(kmeans.inertia_)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Elbow method visualization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bchm5\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bchm5\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1454\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1427\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1428\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[32m   1429\u001b[39m \n\u001b[32m   1430\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1452\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1455\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1464\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_params_vs_input(X)\n\u001b[32m   1466\u001b[39m     random_state = check_random_state(\u001b[38;5;28mself\u001b[39m.random_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bchm5\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bchm5\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bchm5\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bchm5\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nKMeans does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "df = pd.read_csv('clean_data/agged_data2.csv')\n",
    "\n",
    "df[\"prev_wind\"] = (\n",
    "    df[\"prev_wind\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\"mph\", \"\", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df[\"prev_wind\"] = pd.to_numeric(df[\"prev_wind\"], errors=\"coerce\")\n",
    "\n",
    "df[\"prev_travel_magnitude\"] = df[\"prev_travel_magnitude\"].replace({\n",
    "    \"home\": 0,\n",
    "    \"short\": 1,\n",
    "    \"medium\": 2,\n",
    "    \"long\": 3\n",
    "})\n",
    "\n",
    "numeric_cols = [\n",
    "    \"average_humidity_before\",\n",
    "    \"average_wind_before\",\n",
    "    \"prev_humidity\",\n",
    "    \"prev_wind\"\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Run the complete pipeline\n",
    "analysis_results = run_injury_analysis_pipeline(df)\n",
    "\n",
    "# Create visualizations\n",
    "create_visualizations(analysis_results)\n",
    "\n",
    "# Save results\n",
    "analysis_results['data'].to_csv('injury_analysis_results.csv', index=False)\n",
    "print(\"\\nAnalysis complete! Results saved to 'injury_analysis_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fbed78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf30abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA VALIDATION CHECK\n",
      "============================================================\n",
      "\n",
      "Game Status Distribution:\n",
      "  nan: 280024\n",
      "  Questionable: 15455\n",
      "  Out: 10480\n",
      "  Doubtful: 1677\n",
      "\n",
      "Predictor Feature Check:\n",
      "  average_snaps_before: dtype=float64, nulls=13016/307636\n",
      "  sum_travel_magnitude: dtype=float64, nulls=6796/307636\n",
      "  average_weather_before: dtype=float64, nulls=17024/307636\n",
      "COMPLETE INJURY ANALYSIS PIPELINE\n",
      "============================================================\n",
      "\n",
      "[Step 1/3] Preparing data...\n",
      "Found 20 predictor features\n",
      "\n",
      "Step 1: Cleaning predictor features...\n",
      "  Cleaning most_common_surface (object type)...\n",
      "  Cleaning most_common_roof (object type)...\n",
      "  Cleaning prev_humidity (object type)...\n",
      "  Cleaning prev_wind (object type)...\n",
      "  Cleaning prev_surface (object type)...\n",
      "  Cleaning prev_roof (object type)...\n",
      "  Cleaning prev_travel_magnitude (object type)...\n",
      "  Cleaning prev_is_international (object type)...\n",
      "  Cleaning prev_travel_direction (object type)...\n",
      "\n",
      "Step 2: Creating injury target...\n",
      "Injury distribution: {0: 280024, 1: 27612}\n",
      "\n",
      "Step 3: Removing invalid rows...\n",
      "  Removing 6796 rows with all NaN predictors\n",
      "\n",
      "Step 4: Imputing missing values...\n",
      "\n",
      "Final data shape: (300840, 138)\n",
      "Injured: 27085, Not injured: 273755\n",
      "\n",
      "[Step 2/3] Running injury risk model...\n",
      "============================================================\n",
      "ROBUST INJURY RISK MODEL\n",
      "============================================================\n",
      "Data shape: (300840, 20)\n",
      "Target distribution:\n",
      "  0 (Not injured): 273755\n",
      "  1 (Injured): 27085\n",
      "Removing 300840 rows with NaN after conversion\n",
      "\n",
      "Final arrays shape: X=(0, 20), y=(0,)\n",
      "Unique classes in y: []\n",
      "Class counts: []\n",
      "Error: Only one class in target variable\n",
      "\n",
      "[Step 3/3] Analyzing injury patterns...\n",
      "\n",
      "============================================================\n",
      "INJURY PATTERN ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1. Injury Rate by Position:\n",
      "   CB: 56.1% (6643 players)\n",
      "   K: 54.5% (352 players)\n",
      "   S: 53.9% (4420 players)\n",
      "   FB: 53.8% (173 players)\n",
      "   WR: 51.5% (7267 players)\n",
      "   RB: 51.3% (4158 players)\n",
      "   LB: 50.4% (7258 players)\n",
      "   T: 50.0% (4769 players)\n",
      "   TE: 48.8% (3483 players)\n",
      "   G: 46.7% (3463 players)\n",
      "\n",
      "2. Most Common Injury Locations:\n",
      "   Knee: 4234 injuries\n",
      "   Ankle: 3726 injuries\n",
      "   Hamstring: 2923 injuries\n",
      "   Shoulder: 1651 injuries\n",
      "   Concussion: 1600 injuries\n",
      "   Groin: 1139 injuries\n",
      "   Foot: 1131 injuries\n",
      "   Calf: 974 injuries\n",
      "   Back: 930 injuries\n",
      "   Illness: 891 injuries\n",
      "\n",
      "3. Correlation with Injury Status:\n",
      "   average_snaps_before: 0.075\n",
      "   sum_travel_magnitude: nan\n",
      "   sum_elevation_difference: 0.003\n",
      "   average_weather_before: 0.014\n",
      "\n",
      "============================================================\n",
      "ANALYSIS COMPLETE - SUMMARY\n",
      "============================================================\n",
      "✗ Injury Risk Model: FAILED\n",
      "✓ Pattern Analysis: COMPLETE\n",
      "\n",
      "✓ Results saved to 'injury_analysis_complete.csv'\n",
      "\n",
      "✓ Analysis pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION WITH COMPLETE ERROR HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_data_complete(df):\n",
    "    \"\"\"\n",
    "    Complete data preparation with all error handling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define feature sets\n",
    "    predictor_features = [\n",
    "        'average_weather_before', 'average_humidity_before', 'average_wind_before',\n",
    "        'most_common_surface', 'most_common_roof', 'average_snaps_before',\n",
    "        'sum_travel_magnitude', 'sum_tz_diff_magnitude', 'sum_elevation_difference',\n",
    "        'prev_weather', 'prev_humidity', 'prev_wind', 'prev_surface', 'prev_roof',\n",
    "        'prev_snaps', 'prev_travel_magnitude', 'prev_is_international',\n",
    "        'prev_elevation_difference', 'prev_travel_direction', 'prev_elevation_difference_abs_m'\n",
    "    ]\n",
    "    \n",
    "    # Filter to only existing columns\n",
    "    predictor_features = [col for col in predictor_features if col in df.columns]\n",
    "    \n",
    "    print(f\"Found {len(predictor_features)} predictor features\")\n",
    "    \n",
    "    # Step 1: Clean all predictor features\n",
    "    print(\"\\nStep 1: Cleaning predictor features...\")\n",
    "    \n",
    "    for col in predictor_features:\n",
    "        if col in df.columns:\n",
    "            # Check data type\n",
    "            if df[col].dtype == object:\n",
    "                print(f\"  Cleaning {col} (object type)...\")\n",
    "                \n",
    "                # Replace common non-numeric strings\n",
    "                df[col] = df[col].astype(str).str.lower()\n",
    "                non_numeric_strings = ['unknown', 'n/a', 'na', 'nan', 'none', 'null', '']\n",
    "                df[col] = df[col].replace(non_numeric_strings, np.nan)\n",
    "                \n",
    "                # Extract numbers from strings\n",
    "                df[col] = df[col].str.extract(r'([-+]?\\d*\\.?\\d+)')[0]\n",
    "            \n",
    "            # Convert to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Step 2: Create proper binary target\n",
    "    print(\"\\nStep 2: Creating injury target...\")\n",
    "    \n",
    "    if 'game_status' in df.columns:\n",
    "        # Clean and create binary target\n",
    "        df['game_status'] = df['game_status'].astype(str).str.strip().str.lower()\n",
    "        \n",
    "        # Create binary target: 1 for injured, 0 for not injured\n",
    "        injury_keywords = ['out', 'doubtful', 'questionable']\n",
    "        df['is_injured'] = df['game_status'].isin(injury_keywords).astype(int)\n",
    "        \n",
    "        # Ensure we only have 0 and 1\n",
    "        df['is_injured'] = df['is_injured'].clip(0, 1)\n",
    "        \n",
    "        print(f\"Injury distribution: {df['is_injured'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"Warning: No game_status column, creating dummy target\")\n",
    "        df['is_injured'] = 0\n",
    "    \n",
    "    # Step 3: Remove rows with all NaN in predictors\n",
    "    print(\"\\nStep 3: Removing invalid rows...\")\n",
    "    \n",
    "    # Check for rows with all NaN in predictors\n",
    "    nan_mask = df[predictor_features].isna().all(axis=1)\n",
    "    if nan_mask.any():\n",
    "        print(f\"  Removing {nan_mask.sum()} rows with all NaN predictors\")\n",
    "        df = df[~nan_mask]\n",
    "    \n",
    "    # Step 4: Impute missing values\n",
    "    print(\"\\nStep 4: Imputing missing values...\")\n",
    "    \n",
    "    # Use median for numeric columns\n",
    "    for col in predictor_features:\n",
    "        if col in df.columns:\n",
    "            if df[col].isna().any():\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "    \n",
    "    # Final check: ensure all values are finite\n",
    "    for col in predictor_features:\n",
    "        if col in df.columns:\n",
    "            # Replace infinite values\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    print(f\"\\nFinal data shape: {df.shape}\")\n",
    "    print(f\"Injured: {df['is_injured'].sum()}, Not injured: {len(df) - df['is_injured'].sum()}\")\n",
    "    \n",
    "    return df, predictor_features\n",
    "\n",
    "# ============================================================================\n",
    "# ROBUST INJURY RISK MODEL\n",
    "# ============================================================================\n",
    "\n",
    "def robust_injury_risk_model(df, predictor_features):\n",
    "    \"\"\"\n",
    "    Robust injury risk model with full error handling.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ROBUST INJURY RISK MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[predictor_features].copy()\n",
    "    y = df['is_injured'].copy()\n",
    "    \n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    print(f\"Target distribution:\")\n",
    "    print(f\"  0 (Not injured): {sum(y == 0)}\")\n",
    "    print(f\"  1 (Injured): {sum(y == 1)}\")\n",
    "    \n",
    "    # Check if we have enough injured samples\n",
    "    if sum(y == 1) < 20:\n",
    "        print(f\"Warning: Only {sum(y == 1)} injured samples, model may not be reliable\")\n",
    "    \n",
    "    # Ensure X is numeric and has no NaN\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_array = X.values.astype(float)\n",
    "    y_array = y.values.astype(int)\n",
    "    \n",
    "    # Remove any rows with NaN in X_array\n",
    "    nan_mask = np.isnan(X_array).any(axis=1)\n",
    "    if nan_mask.any():\n",
    "        print(f\"Removing {nan_mask.sum()} rows with NaN after conversion\")\n",
    "        X_array = X_array[~nan_mask]\n",
    "        y_array = y_array[~nan_mask]\n",
    "    \n",
    "    print(f\"\\nFinal arrays shape: X={X_array.shape}, y={y_array.shape}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    unique_classes, class_counts = np.unique(y_array, return_counts=True)\n",
    "    print(f\"Unique classes in y: {unique_classes}\")\n",
    "    print(f\"Class counts: {class_counts}\")\n",
    "    \n",
    "    # We need at least 2 classes\n",
    "    if len(unique_classes) < 2:\n",
    "        print(\"Error: Only one class in target variable\")\n",
    "        return {}\n",
    "    \n",
    "    # Split data\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_array, y_array, test_size=0.2, random_state=42, stratify=y_array\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Stratified split failed: {e}\")\n",
    "        print(\"Using regular split...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_array, y_array, test_size=0.2, random_state=42\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nTrain: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train models\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Train\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = model.predict(X_test_scaled)\n",
    "            \n",
    "            # Evaluate\n",
    "            accuracy = model.score(X_test_scaled, y_test)\n",
    "            \n",
    "            print(f\"Accuracy: {accuracy:.3f}\")\n",
    "            print(\"Classification Report:\")\n",
    "            \n",
    "            # Handle class labels properly\n",
    "            class_labels = ['Not Injured', 'Injured']\n",
    "            print(classification_report(y_test, y_pred, target_names=class_labels, zero_division=0))\n",
    "            \n",
    "            # Feature importance\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'feature': predictor_features,\n",
    "                    'importance': importances\n",
    "                }).sort_values('importance', ascending=False)\n",
    "                \n",
    "                print(\"\\nTop 10 Most Important Features:\")\n",
    "                print(feature_importance.head(10).to_string(index=False))\n",
    "                \n",
    "                # Plot feature importance\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.barh(feature_importance.head(10)['feature'], \n",
    "                        feature_importance.head(10)['importance'])\n",
    "                plt.xlabel('Importance')\n",
    "                plt.title(f'{name} - Top 10 Injury Risk Features')\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'feature_importance': feature_importance,\n",
    "                    'scaler': scaler,\n",
    "                    'accuracy': accuracy\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLE ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_injury_patterns(df):\n",
    "    \"\"\"\n",
    "    Simple analysis of injury patterns.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"INJURY PATTERN ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Injury rate by position\n",
    "    if 'position' in df.columns and 'is_injured' in df.columns:\n",
    "        injury_by_position = df.groupby('position')['is_injured'].agg(['mean', 'count'])\n",
    "        injury_by_position = injury_by_position.sort_values('mean', ascending=False)\n",
    "        \n",
    "        print(\"\\n1. Injury Rate by Position:\")\n",
    "        for position, row in injury_by_position.head(10).iterrows():\n",
    "            print(f\"   {position}: {row['mean']:.1%} ({int(row['count'])} players)\")\n",
    "        \n",
    "        results['injury_by_position'] = injury_by_position\n",
    "    \n",
    "    # 2. Most common injury locations\n",
    "    if 'injury_lcoation' in df.columns:\n",
    "        injury_locations = df['injury_lcoation'].value_counts().head(10)\n",
    "        \n",
    "        print(\"\\n2. Most Common Injury Locations:\")\n",
    "        for location, count in injury_locations.items():\n",
    "            print(f\"   {location}: {count} injuries\")\n",
    "        \n",
    "        results['common_injury_locations'] = injury_locations\n",
    "    \n",
    "    # 3. Correlation with numeric features\n",
    "    numeric_features = ['average_snaps_before', 'sum_travel_magnitude', \n",
    "                       'sum_elevation_difference', 'average_weather_before']\n",
    "    \n",
    "    if 'is_injured' in df.columns:\n",
    "        print(\"\\n3. Correlation with Injury Status:\")\n",
    "        for feature in numeric_features:\n",
    "            if feature in df.columns:\n",
    "                try:\n",
    "                    # Ensure numeric\n",
    "                    feature_series = pd.to_numeric(df[feature], errors='coerce')\n",
    "                    corr = feature_series.corr(df['is_injured'])\n",
    "                    print(f\"   {feature}: {corr:.3f}\")\n",
    "                except:\n",
    "                    print(f\"   {feature}: Could not calculate correlation\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_complete_analysis(df):\n",
    "    \"\"\"\n",
    "    Complete analysis pipeline with full error handling.\n",
    "    \"\"\"\n",
    "    print(\"COMPLETE INJURY ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Prepare data\n",
    "        print(\"\\n[Step 1/3] Preparing data...\")\n",
    "        df_prepared, predictor_features = prepare_data_complete(df)\n",
    "        \n",
    "        # Step 2: Run injury risk model\n",
    "        print(\"\\n[Step 2/3] Running injury risk model...\")\n",
    "        model_results = robust_injury_risk_model(df_prepared, predictor_features)\n",
    "        \n",
    "        # Step 3: Analyze patterns\n",
    "        print(\"\\n[Step 3/3] Analyzing injury patterns...\")\n",
    "        analysis_results = analyze_injury_patterns(df_prepared)\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ANALYSIS COMPLETE - SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if model_results:\n",
    "            print(\"✓ Injury Risk Model: SUCCESS\")\n",
    "            for name, result in model_results.items():\n",
    "                print(f\"  - {name} Accuracy: {result['accuracy']:.3f}\")\n",
    "                top_feature = result['feature_importance'].iloc[0]\n",
    "                print(f\"  - Top predictor: {top_feature['feature']} (importance: {top_feature['importance']:.3f})\")\n",
    "        else:\n",
    "            print(\"✗ Injury Risk Model: FAILED\")\n",
    "        \n",
    "        print(\"✓ Pattern Analysis: COMPLETE\")\n",
    "        \n",
    "        # Save results\n",
    "        df_prepared.to_csv('injury_analysis_complete.csv', index=False)\n",
    "        print(\"\\n✓ Results saved to 'injury_analysis_complete.csv'\")\n",
    "        \n",
    "        return {\n",
    "            'data': df_prepared,\n",
    "            'model_results': model_results,\n",
    "            'analysis_results': analysis_results,\n",
    "            'predictor_features': predictor_features\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Analysis failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# DEBUGGING AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_data(df):\n",
    "    \"\"\"\n",
    "    Validate data before running analysis.\n",
    "    \"\"\"\n",
    "    print(\"DATA VALIDATION CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['game_status']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"✗ Missing required columns: {missing_cols}\")\n",
    "        return False\n",
    "    \n",
    "    # Check game_status values\n",
    "    print(\"\\nGame Status Distribution:\")\n",
    "    if 'game_status' in df.columns:\n",
    "        status_counts = df['game_status'].value_counts(dropna=False)\n",
    "        for status, count in status_counts.head(10).items():\n",
    "            print(f\"  {status}: {count}\")\n",
    "    \n",
    "    # Check predictor features\n",
    "    predictor_features = [\n",
    "        'average_snaps_before', 'sum_travel_magnitude', 'average_weather_before'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nPredictor Feature Check:\")\n",
    "    for feature in predictor_features:\n",
    "        if feature in df.columns:\n",
    "            null_count = df[feature].isna().sum()\n",
    "            dtype = df[feature].dtype\n",
    "            print(f\"  {feature}: dtype={dtype}, nulls={null_count}/{len(df)}\")\n",
    "        else:\n",
    "            print(f\"  {feature}: NOT FOUND\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = pd.read_csv('clean_data/agged_data2.csv')\n",
    "    \n",
    "    # Validate data first\n",
    "    if validate_data(df):\n",
    "        # Run complete analysis\n",
    "        results = run_complete_analysis(df)\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\n✓ Analysis pipeline completed successfully!\")\n",
    "        else:\n",
    "            print(\"\\n✗ Analysis pipeline failed\")\n",
    "    else:\n",
    "        print(\"\\n✗ Data validation failed, cannot proceed with analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
