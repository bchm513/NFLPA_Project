{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a989d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bchm5\\AppData\\Local\\Temp\\ipykernel_6148\\108630562.py:4: DtypeWarning: Columns (19,87,89,95,127,143) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  left_merged_data = pd.read_csv('../data_prep/data/left_merged_data.csv')\n",
      "C:\\Users\\bchm5\\AppData\\Local\\Temp\\ipykernel_6148\\108630562.py:5: DtypeWarning: Columns (13,19,87,89,95,127,143) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  right_merged_data = pd.read_csv('../data_prep/data/right_merged_data.csv')\n",
      "C:\\Users\\bchm5\\AppData\\Local\\Temp\\ipykernel_6148\\108630562.py:7: DtypeWarning: Columns (87,89,95) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  outer_merged_data = pd.read_csv('../data_prep/data/outer_merged_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "import pandas as pd\n",
    "\n",
    "left_merged_data = pd.read_csv('../data_prep/data/left_merged_data.csv')\n",
    "right_merged_data = pd.read_csv('../data_prep/data/right_merged_data.csv')\n",
    "inner_merged_data = pd.read_csv('../data_prep/data/inner_merged_data.csv')\n",
    "outer_merged_data = pd.read_csv('../data_prep/data/outer_merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ce4105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b80a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_merged_data\n",
    "# right_merged_data\n",
    "# inner_merged_data\n",
    "# outer_merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a052e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in left_merged_data.columns:\n",
    "#     print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a6a8244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Remove duplicate columns from DataFrame as specified.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with specified duplicate columns removed\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # List of all columns to remove\n",
    "    columns_to_remove = [\n",
    "        # Auto-generated indices\n",
    "        'Unnamed: 0',\n",
    "        'index',\n",
    "        \n",
    "        # Duplicate stats (keeping _x versions, removing _y versions)\n",
    "        'pass_cmp_y',\n",
    "        'rush_att_y',\n",
    "        'rush_yds_y', \n",
    "        'rush_td_y',\n",
    "        'targets_y',\n",
    "        'rec_y',\n",
    "        'rec_yds_y',\n",
    "        'rec_td_y',\n",
    "        'def_int_y',\n",
    "        'sacks_y',\n",
    "        'tackles_combined_y',\n",
    "        \n",
    "        # Duplicate without suffix\n",
    "        ' pass_att',  # with leading space\n",
    "        \n",
    "        # Keep elevation, remove stadium (or vice versa - choose one)\n",
    "        'team_home_stadium',  # Remove this, keep team_home_elevation\n",
    "    ]\n",
    "    \n",
    "    # Remove the columns\n",
    "    df_clean = df_clean.drop(columns=columns_to_remove, errors='ignore')\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17ee3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(df):\n",
    "    # Method 1: Using pandas to_datetime with custom format\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%A %b %d %Y', errors='coerce')\n",
    "\n",
    "    print(\"Sample conversions:\")\n",
    "    print(df['date'].head(10))\n",
    "    print(f\"\\nNumber of failed conversions: {df['date'].isna().sum()}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e48ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample conversions:\n",
      "0   2016-09-08\n",
      "1   2016-09-08\n",
      "2   2016-09-08\n",
      "3   2016-09-08\n",
      "4   2016-09-08\n",
      "5   2016-09-08\n",
      "6   2016-09-08\n",
      "7   2016-09-08\n",
      "8   2016-09-08\n",
      "9   2016-09-08\n",
      "Name: date, dtype: datetime64[ns]\n",
      "\n",
      "Number of failed conversions: 0\n",
      "Sample conversions:\n",
      "0   2016-09-08\n",
      "1   2016-09-08\n",
      "2   2016-09-08\n",
      "3   2016-09-08\n",
      "4   2016-09-08\n",
      "5   2016-09-08\n",
      "6   2016-09-08\n",
      "7   2016-09-08\n",
      "8   2016-09-08\n",
      "9   2016-09-08\n",
      "Name: date, dtype: datetime64[ns]\n",
      "\n",
      "Number of failed conversions: 0\n",
      "Sample conversions:\n",
      "0   2016-09-08\n",
      "1   2016-09-08\n",
      "2   2016-09-08\n",
      "3   2016-09-08\n",
      "4   2016-09-08\n",
      "5   2016-09-08\n",
      "6   2016-09-08\n",
      "7   2016-09-11\n",
      "8   2016-09-11\n",
      "9   2016-09-11\n",
      "Name: date, dtype: datetime64[ns]\n",
      "\n",
      "Number of failed conversions: 0\n",
      "Sample conversions:\n",
      "0   2023-12-11\n",
      "1   2021-12-13\n",
      "2   2023-12-25\n",
      "3   2016-12-26\n",
      "4   2018-12-03\n",
      "5   2022-01-17\n",
      "6   2021-11-15\n",
      "7   2016-11-21\n",
      "8   2020-11-23\n",
      "9   2025-11-24\n",
      "Name: date, dtype: datetime64[ns]\n",
      "\n",
      "Number of failed conversions: 0\n"
     ]
    }
   ],
   "source": [
    "updated_left_merged_data = to_datetime(left_merged_data)\n",
    "updated_right_merged_data = to_datetime(right_merged_data)\n",
    "updated_inner_merged_data = to_datetime(inner_merged_data)\n",
    "updated_outer_merged_data = to_datetime(outer_merged_data)\n",
    "\n",
    "clean_left_merged_data = remove_duplicate_columns(updated_left_merged_data)\n",
    "clean_right_merged_data = remove_duplicate_columns(updated_right_merged_data)\n",
    "clean_inner_merged_data = remove_duplicate_columns(updated_inner_merged_data)\n",
    "clean_outer_merged_data = remove_duplicate_columns(updated_outer_merged_data)\n",
    "\n",
    "org_left_merged_data = clean_left_merged_data.sort_values(['player_name','date'])\n",
    "org_right_merged_data = clean_right_merged_data.sort_values(['player_name','date'])\n",
    "org_inner_merged_data = clean_inner_merged_data.sort_values(['player_name','date'])\n",
    "org_outer_merged_data = clean_outer_merged_data.sort_values(['player_name','date'])\n",
    "\n",
    "# print(len(right_merged_data.columns))\n",
    "# print(len(clean_right_merged_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72e77021",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_left_merged_data.to_csv('clean_data/clean_left_merged_data.csv')\n",
    "org_outer_merged_data.to_csv('clean_data/clean_outer_merged_data.csv')\n",
    "org_inner_merged_data.to_csv('clean_data/clean_inner_merged_data.csv')\n",
    "org_right_merged_data.to_csv('clean_data/clean_right_merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7471ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bchm5\\AppData\\Local\\Temp\\ipykernel_6148\\1886368649.py:1: DtypeWarning: Columns (84,86,92) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_outer_merged_data = pd.read_csv('clean_data/clean_outer_merged_data.csv')\n"
     ]
    }
   ],
   "source": [
    "final_outer_merged_data = pd.read_csv('clean_data/clean_outer_merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13474baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6020\n"
     ]
    }
   ],
   "source": [
    "# clean_left_merged_data['injury_lcoation'].unique()\n",
    "print(len(final_outer_merged_data['player_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51a11059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(df):\n",
    "\n",
    "    # Helper function to safely convert to numeric\n",
    "    def safe_numeric(series):\n",
    "        \"\"\"Convert series to numeric, coerce errors to NaN\"\"\"\n",
    "        return pd.to_numeric(series, errors='coerce')\n",
    "    \n",
    "    # Columns to group by for injury context\n",
    "    groupby_cols = [\"player_name\", \"date\", \"position\",\n",
    "                    \"injury_lcoation\", \"practice_status\", \"game_status\t\t\"]\n",
    "    \n",
    "    # Environment columns for \"before injury\" analysis\n",
    "    environment_cols = [\n",
    "        \"weather_content\", \"surface\", \"temperature\", \"humidity\", \"wind\", \"roof\",\n",
    "        \"time_zone_offset\", \"is_international\", \"team_timezone_offset\", \n",
    "        \"team_timezone\", \"game_tz_difference\", \"travel_direction\", \n",
    "        \"tz_diff_magnitude\", \"travel_magnitude\", \"is_long_travel\", \n",
    "        \"travel_description\", \"game_stadium_elevation\", \"team_home_elevation\",\n",
    "        \"elevation_difference_m\", \"elevation_difference_abs_m\",\n",
    "        \"snap_count\"\n",
    "    ]\n",
    "    \n",
    "    # Production columns for \"after injury\" analysis  \n",
    "    production_cols = [\n",
    "        # Basic offensive stats\n",
    "        \"pass_cmp_x\", \"pass_att\", \"pass_yds\", \"pass_td\", \"pass_int\", \n",
    "        \"pass_sacked_x\", \"pass_sacked_yds\", \"pass_long\", \"pass_rating\",\n",
    "        \"rush_att_x\", \"rush_yds_x\", \"rush_td_x\", \"rush_long\",\n",
    "        \"targets_x\", \"rec_x\", \"rec_yds_x\", \"rec_td_x\", \"rec_long\",\n",
    "        \"fumbles\", \"fumbles_lost\",\n",
    "        \n",
    "        # Defensive stats\n",
    "        \"def_int_x\", \"def_int_yds\", \"def_int_td\", \"def_int_long\", \"pass_defended\",\n",
    "        \"sacks_x\", \"tackles_combined_x\", \"tackles_solo\", \"tackles_assists\", \n",
    "        \"tackles_loss\", \"qb_hits\",\n",
    "        \"fumbles_rec\", \"fumbles_rec_yds\", \"fumbles_rec_td\", \"fumbles_forced\",\n",
    "        \n",
    "        # Special teams\n",
    "        \"xpm\", \"xpa\", \"fgm\", \"fga\",\n",
    "        \"punt\", \"punt_yds\", \"punt_yds_per_punt\", \"punt_long\",\n",
    "        \"kick_ret\", \"kick_ret_yds\", \"kick_ret_yds_per_ret\", \"kick_ret_td\", \"kick_ret_long\",\n",
    "        \"punt_ret\", \"punt_ret_yds\", \"punt_ret_yds_per_ret\", \"punt_ret_td\", \"punt_ret_long\",\n",
    "        \n",
    "        # Advanced offensive stats\n",
    "        \"pass_first_down\", \"pass_first_down_pct\", \"pass_target_yds\", \"pass_tgt_yds_per_att\",\n",
    "        \"pass_air_yds\", \"pass_air_yds_per_cmp\", \"pass_air_yds_per_att\",\n",
    "        \"pass_yac\", \"pass_yac_per_cmp\", \"pass_drops\", \"pass_drop_pct\",\n",
    "        \"pass_poor_throws\", \"pass_poor_throw_pct\", \"pass_sacked_y\", \"pass_blitzed\", \"pass_hurried\",\n",
    "        \"pass_hits\", \"pass_pressured\", \"pass_pressured_pct\", \"rush_scrambles\", \"rush_scrambles_yds_per_att\",\n",
    "        \"rush_first_down\", \"rush_yds_before_contact\", \"rush_yds_bc_per_rush\", \"rush_yac\", \"rush_yac_per_rush\",\n",
    "        \"rush_broken_tackles\", \"rush_broken_tackles_per_rush\",\n",
    "        \"rec_first_down\", \"rec_air_yds\", \"rec_air_yds_per_rec\", \"rec_yac\", \"rec_yac_per_rec\", \"rec_adot\",\n",
    "        \"rec_broken_tackles\", \"rec_broken_tackles_per_rec\", \"rec_drops\", \"rec_drop_pct\", \n",
    "        \"rec_target_int\", \"rec_pass_rating\",\n",
    "        \n",
    "        # Advanced defensive stats\n",
    "        \"def_targets\", \"def_cmp\", \"def_cmp_perc\", \"def_cmp_yds\", \"def_yds_per_cmp\",\n",
    "        \"def_yds_per_target\", \"def_cmp_td\", \"def_pass_rating\", \"def_tgt_yds_per_att\",\n",
    "        \"def_air_yds\", \"def_yac\",\n",
    "        \"blitzes\", \"qb_hurry\", \"qb_knockdown\", \"pressures\", \"tackles_missed\", \"tackles_missed_pct\",\n",
    "        \n",
    "        \"snap_count\"\n",
    "    ]\n",
    "    \n",
    "    # Filter to only include columns that exist in the dataframe\n",
    "    environment_cols = [col for col in environment_cols if col in df.columns]\n",
    "    production_cols = [col for col in production_cols if col in df.columns]\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for name in df['player_name'].unique():\n",
    "        count += 1\n",
    "        print(f\"Processing {name}... {count}/{len(final_outer_merged_data['player_name'].unique())}\")\n",
    "\n",
    "        current_name_df = df[df['player_name'] == name]\n",
    "        # print(current_name_df.head())\n",
    "\n",
    "        for index, row in current_name_df.iterrows():\n",
    "\n",
    "            group_row = []\n",
    "            for col in groupby_cols:\n",
    "                group_row.append(row[col])\n",
    "\n",
    "            # print(group_row)\n",
    "\n",
    "            # set injury date\n",
    "            injury_date = row['date']\n",
    "            # print(injury_date)\n",
    "\n",
    "            # Get previous games (before injury)\n",
    "            before_games = current_name_df[current_name_df['date'] < injury_date]\n",
    "            \n",
    "            # Get current and future games (after injury, including injury game)\n",
    "            after_games = current_name_df[current_name_df['date'] >= injury_date]\n",
    "            \n",
    "            # Get most recent previous game (the one right before injury)\n",
    "            prev_game = before_games.iloc[-1] if len(before_games) > 0 else pd.Series()\n",
    "\n",
    "            # print(before_games)\n",
    "            # print(after_games)\n",
    "            # print(prev_game)\n",
    "            \n",
    "            if len(before_games) > 0:\n",
    "\n",
    "                ##### environment variables\n",
    "                # get temperature\n",
    "                try:\n",
    "                    average_weather_before = before_games[\"temperature\"].mean()\n",
    "                except Exception:\n",
    "                    average_weather_before = np.nan\n",
    "                # get humidity\n",
    "                try:\n",
    "                    average_humidity_before = before_games[\"humidity\"].mean()\n",
    "                except Exception:\n",
    "                    average_humidity_before = np.nan\n",
    "                # get wind\n",
    "                try:\n",
    "                    average_wind_before = before_games[\"wind\"].mean()\n",
    "                except Exception:\n",
    "                    average_wind_before = np.nan\n",
    "                try:\n",
    "                    most_common_surface = before_games[\"surface\"].mode().iloc[0]  # get surface\n",
    "                except Exception:\n",
    "                    most_common_surface = np.nan\n",
    "                try:\n",
    "                    most_common_roof = before_games[\"roof\"].mode().iloc[0] # get roof\n",
    "                except Exception:\n",
    "                    most_common_roof = np.nan\n",
    "\n",
    "                ##### snap workload\n",
    "                average_snaps_before = before_games['snap_count'].mean() # get snap count\n",
    "\n",
    "                ##### travel data\n",
    "                travel_magnitude_numeric = pd.to_numeric(before_games['travel_magnitude'], errors='coerce')\n",
    "                tz_diff_numeric = pd.to_numeric(before_games['tz_diff_magnitude'], errors='coerce')\n",
    "                elevation_diff_numeric = pd.to_numeric(before_games['elevation_difference_abs_m'], errors='coerce')\n",
    "\n",
    "                sum_travel_magnitude = travel_magnitude_numeric.sum()\n",
    "                sum_tz_diff_magnitude = tz_diff_numeric.abs().sum()  # Already numeric\n",
    "                sum_elevation_difference = elevation_diff_numeric.sum()\n",
    "\n",
    "                ###################### previous game ######################\n",
    "                ##### environment data\n",
    "                try:\n",
    "                    prev_weather = prev_game[\"temperature\"]\n",
    "                except Exception:\n",
    "                    prev_weather = np.nan\n",
    "                # get humidity\n",
    "                try:\n",
    "                    prev_humidity = prev_game[\"humidity\"]\n",
    "                except Exception:\n",
    "                    prev_humidity = np.nan\n",
    "                # get wind\n",
    "                try:\n",
    "                    prev_wind = prev_game[\"wind\"]\n",
    "                except Exception:\n",
    "                    prev_wind = np.nan\n",
    "                prev_surface = prev_game[\"surface\"]  # get surface\n",
    "                prev_roof = prev_game[\"roof\"] # get roof\n",
    "\n",
    "                ##### snap workload\n",
    "                prev_snaps = prev_game['snap_count'] # get snap count\n",
    "\n",
    "                ##### travel data\n",
    "                prev_travel_magnitude = prev_game['travel_magnitude']\n",
    "                prev_elevation_difference = prev_game['elevation_difference_abs_m']\n",
    "                prev_travel_direction = prev_game['travel_direction']\n",
    "                prev_elevation_difference_abs_m = prev_game['elevation_difference_abs_m']\n",
    "                prev_is_international = prev_game[\"is_international\"]\n",
    "\n",
    "                prev_game_content = [prev_weather,prev_humidity,prev_wind,prev_surface,prev_roof,\n",
    "                                        prev_snaps,\n",
    "                                        prev_travel_magnitude,prev_is_international,prev_elevation_difference,prev_travel_direction,prev_elevation_difference_abs_m]\n",
    "\n",
    "            else:\n",
    "                average_weather_before = np.nan\n",
    "                average_humidity_before = np.nan\n",
    "                average_wind_before = np.nan\n",
    "                most_common_surface = np.nan\n",
    "                most_common_roof = np.nan\n",
    "\n",
    "                average_snaps_before = np.nan\n",
    "\n",
    "                sum_travel_magnitude = np.nan\n",
    "                sum_elevation_difference = np.nan\n",
    "                sum_tz_diff_magnitude = np.nan\n",
    "\n",
    "                prev_game_content = [np.nan,np.nan,np.nan,np.nan,np.nan,\n",
    "                                        np.nan,\n",
    "                                        np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "\n",
    "            before_content = [average_weather_before, average_humidity_before, average_wind_before, most_common_surface, most_common_roof,\n",
    "                                average_snaps_before,\n",
    "                                sum_travel_magnitude, sum_tz_diff_magnitude, sum_elevation_difference]\n",
    "            # print(before_content)\n",
    "\n",
    "            if len(after_games) > 0:\n",
    "                after_content = []\n",
    "\n",
    "                for col in production_cols:\n",
    "                    clean_series = pd.to_numeric(after_games[col], errors='coerce')\n",
    "                    after_content.append(clean_series.mean())\n",
    "                # print(len(after_content))\n",
    "\n",
    "            else:\n",
    "                for x in range(107):\n",
    "                    after_content.append(np.nan)\n",
    "\n",
    "            # print(after_content)\n",
    "\n",
    "            ##### create total row\n",
    "\n",
    "            row_to_write = group_row + before_content + prev_game_content + after_content\n",
    "            # print(row_to_write)\n",
    "\n",
    "            ##### write the row\n",
    "            with open('clean_data/agged_data2.csv', 'a', newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row_to_write)\n",
    "                \n",
    "        # break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda472f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing A'Shawn Robinson... 1/6020\n",
      "Processing A.J. Bouye... 2/6020\n",
      "Processing A.J. Brown... 3/6020\n",
      "Processing A.J. Cann... 4/6020\n",
      "Processing A.J. Derby... 5/6020\n",
      "Processing A.J. Epenesa... 6/6020\n",
      "Processing A.J. Francis... 7/6020\n",
      "Processing A.J. Green... 8/6020\n",
      "Processing A.J. Hawk... 9/6020\n",
      "Processing A.J. Klein... 10/6020\n",
      "Processing A.J. McCarron... 11/6020\n",
      "Processing A.J. Moore... 12/6020\n",
      "Processing A.J. Terrell... 13/6020\n",
      "Processing A.J. Thomas... 14/6020\n",
      "Processing A.Q. Shipley... 15/6020\n",
      "Processing A.T. Perry... 16/6020\n",
      "Processing AJ Arcuri... 17/6020\n",
      "Processing AJ Barner... 18/6020\n",
      "Processing AJ Cole... 19/6020\n",
      "Processing AJ Cole III... 20/6020\n",
      "Processing AJ Dillon... 21/6020\n",
      "Processing AJ Finley... 22/6020\n",
      "Processing AJ McCarron... 23/6020\n",
      "Processing AJ Parker... 24/6020\n",
      "Processing AJ Terrell... 25/6020\n",
      "Processing Aaron Adeoye... 26/6020\n",
      "Processing Aaron Banks... 27/6020\n",
      "Processing Aaron Brewer... 28/6020\n",
      "Processing Aaron Burbridge... 29/6020\n",
      "Processing Aaron Colvin... 30/6020\n",
      "Processing Aaron Crawford... 31/6020\n",
      "Processing Aaron Donald... 32/6020\n",
      "Processing Aaron Fuller... 33/6020\n",
      "Processing Aaron Jones... 34/6020\n",
      "Processing Aaron Lynch... 35/6020\n",
      "Processing Aaron Parker... 36/6020\n",
      "Processing Aaron Patrick... 37/6020\n",
      "Processing Aaron Ripkowski... 38/6020\n",
      "Processing Aaron Robinson... 39/6020\n",
      "Processing Aaron Rodgers... 40/6020\n",
      "Processing Aaron Shampklin... 41/6020\n",
      "Processing Aaron Stinnie... 42/6020\n",
      "Processing Aaron Wallace... 43/6020\n",
      "Processing Aaron Williams... 44/6020\n",
      "Processing Abdul Carter... 45/6020\n",
      "Processing Abdullah Anderson... 46/6020\n",
      "Processing Abraham Beauplan... 47/6020\n",
      "Processing Abraham Lucas... 48/6020\n",
      "Processing Abry Jones... 49/6020\n"
     ]
    }
   ],
   "source": [
    "aggregate_data(final_outer_merged_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
